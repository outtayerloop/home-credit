{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ed90e31",
   "metadata": {
    "id": "5ed90e31"
   },
   "source": [
    "# XGboost model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5013a5f",
   "metadata": {
    "id": "a5013a5f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import logging\n",
    "import mlflow\n",
    "from urllib.parse import urlparse\n",
    "import xgboost as xgb\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d6ca88",
   "metadata": {
    "id": "d1d6ca88"
   },
   "source": [
    "## Splitting dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "738f427f",
   "metadata": {
    "id": "738f427f"
   },
   "outputs": [],
   "source": [
    "def get_split_train_data():\n",
    "  \"\"\"Return a tuple containing split train data into X_train X_test, y_train and y_test.\"\"\"\n",
    "  df = pd.read_csv('../../data/processed/processed_application_train.csv')\n",
    "  train, test = train_test_split(df)\n",
    "  X_train = train.drop(['TARGET'], axis=1)\n",
    "  X_test = test.drop(['TARGET'], axis=1)\n",
    "  y_train = train[['TARGET']]\n",
    "  y_test = test[['TARGET']]\n",
    "  return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931a8c69",
   "metadata": {
    "id": "931a8c69"
   },
   "source": [
    "## Adding MLFLow workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mXVizP3vXWdc",
   "metadata": {
    "id": "mXVizP3vXWdc"
   },
   "source": [
    "### Configuring logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ElavYC5wXOrF",
   "metadata": {
    "id": "ElavYC5wXOrF"
   },
   "outputs": [],
   "source": [
    "def get_configured_logger():\n",
    "  \"\"\"Return a logger for console outputs configured to print warnings.\"\"\"\n",
    "  logging.basicConfig(level=logging.WARN)\n",
    "  return logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hFmZgG_hXvgi",
   "metadata": {
    "id": "hFmZgG_hXvgi"
   },
   "source": [
    "### Training model on split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "Jod5BBLEX0qJ",
   "metadata": {
    "id": "Jod5BBLEX0qJ"
   },
   "outputs": [],
   "source": [
    "def train_xgboost_classifier(X_train, y_train):\n",
    "  \"\"\"Return XGBClassifier fit on input ndarrays X_train and y_train.\n",
    "\n",
    "  Keyword arguments:\n",
    "  X_train -- ndarray containing all train columns except target column\n",
    "  y_train -- ndarray target column values to train the model\n",
    "  \"\"\"\n",
    "  clf = xgb.XGBClassifier(scale_pos_weight=(1 - y_train.values.mean()), n_jobs=-1)\n",
    "  grid_search = GridSearchCV(clf, {'max_depth': [5, 10], 'min_child_weight': [5, 10], 'n_estimators': [25]}, n_jobs=-1, cv=5, scoring='accuracy')\n",
    "  grid_search.fit(X_train.values, y_train.values)\n",
    "  clf.set_params(**grid_search.best_params_)\n",
    "  clf = clf.fit(X_train, y_train)\n",
    "  return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZtpOwQmScTqs",
   "metadata": {
    "id": "ZtpOwQmScTqs"
   },
   "source": [
    "### Getting model evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30ac1411",
   "metadata": {
    "id": "30ac1411"
   },
   "outputs": [],
   "source": [
    "def eval_metrics(actual, pred):\n",
    "  \"\"\"Return a tuple containing model classification accuracy, confusion matrix and f1_score.\n",
    "\n",
    "  Keyword arguments:\n",
    "  actual -- ndarray y_test containing true target values\n",
    "  pred -- ndarray of the predicted target values by the model\n",
    "  \"\"\"\n",
    "  accuracy = accuracy_score(actual, pred)\n",
    "  conf_matrix = confusion_matrix(actual, pred)\n",
    "  f_score = f1_score(actual, pred)\n",
    "  return accuracy, conf_matrix, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "xlD2NDs7Yl52",
   "metadata": {
    "id": "xlD2NDs7Yl52"
   },
   "outputs": [],
   "source": [
    "def get_model_evaluation_metrics(clf, X_test, y_test):\n",
    "  \"\"\"Return a tuple containing model classification accuracy, confusion matrix, f1_score and ROC area under the curve score.\n",
    "  \n",
    "  Keyword arguments:\n",
    "  clf -- classifier model\n",
    "  X_test -- ndarray containing all test columns except target column\n",
    "  y_test -- ndarray target column values to test the model\n",
    "  \"\"\"\n",
    "  predicted_repayments = clf.predict(X_test)\n",
    "  (accuracy, conf_matrix, f_score) = eval_metrics(y_test, predicted_repayments)\n",
    "  xgb_probs = clf.predict_proba(X_test)\n",
    "  xgb_probs = xgb_probs[:, 0]  # keeping only the first class (repayment OK)\n",
    "  xgb_roc_auc_score = roc_auc_score(y_test, xgb_probs)\n",
    "  random_probs = [0 for _ in range(len(y_test))]\n",
    "  random_roc_auc_score = roc_auc_score(y_test, random_probs)\n",
    "  return accuracy, conf_matrix, f_score, xgb_roc_auc_score, random_roc_auc_score, xgb_probs, random_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xIhVvhSMcbRN",
   "metadata": {
    "id": "xIhVvhSMcbRN"
   },
   "source": [
    "### Tracking model on MLFLow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "o82vjjLVb1NC",
   "metadata": {
    "id": "o82vjjLVb1NC"
   },
   "outputs": [],
   "source": [
    "def track_model_params(clf):\n",
    "  \"\"\"Log model params on MLFlow UI.\n",
    "\n",
    "  Keyword arguments:\n",
    "  clf -- classifier model\n",
    "  \"\"\"\n",
    "  clf_params = clf.get_params()\n",
    "  for param in clf_params:\n",
    "      param_value = clf_params[param]\n",
    "      mlflow.log_param(param, param_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502465d1",
   "metadata": {},
   "source": [
    "## Vizualizing ROC AUC scores summaries for both XGBoost and Random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "092a4b32",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def vizualize_roc_curves(xgb_roc_auc_score, random_roc_auc_score, y_test, xgb_probs, random_probs):\n",
    "  \"\"\"Vizualize ROC curves for both fit model and random model.\n",
    "\n",
    "  Keyword arguments:\n",
    "  xgb_roc_auc_score -- fit model ROC AUC score\n",
    "  random_roc_auc_score -- random model ROC AUC score\n",
    "  y_test -- ndarray of target values\n",
    "  xgb_probs -- fit model predicted probabilities\n",
    "  random_probs -- random model predicted probabilities\n",
    "  \"\"\"\n",
    "  # summarize scores\n",
    "  print('Random model: ROC AUC=%.3f' % random_roc_auc_score)\n",
    "  print('XGBoost: ROC AUC=%.3f' % xgb_roc_auc_score)\n",
    "  # calculate roc curves\n",
    "  random_fpr, random_tpr, _ = roc_curve(y_test, random_probs)\n",
    "  xgb_fpr, xgb_tpr, _ = roc_curve(y_test, xgb_probs)\n",
    "  # plot the roc curve for the model\n",
    "  pyplot.plot(random_fpr, random_tpr, linestyle='--', label='Random model')\n",
    "  pyplot.plot(xgb_fpr, xgb_tpr, marker='.', label='XGBoost')\n",
    "  # axis labels\n",
    "  pyplot.xlabel('False Positive Rate')\n",
    "  pyplot.ylabel('True Positive Rate')\n",
    "  # show the legend\n",
    "  pyplot.legend()\n",
    "  # show the plot\n",
    "  pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41845fd2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def track_model_metrics(clf, X_test, y_test):\n",
    "  \"\"\"Log model metrics on MLFlow UI.\n",
    "  \n",
    "  Keyword arguments:\n",
    "  clf -- classifier model\n",
    "  X_test -- ndarray containing all test columns except target column\n",
    "  y_test -- ndarray target column values to test the model\n",
    "  \"\"\"\n",
    "  (accuracy, conf_matrix, f_score, xgb_roc_auc_score, random_roc_auc_score, xgb_probs, random_probs) = \\\n",
    "    get_model_evaluation_metrics(clf, X_test, y_test)\n",
    "  mlflow.log_metric('accuracy', accuracy)\n",
    "  mlflow.log_metric('f1_score', f_score)\n",
    "  mlflow.log_metric('roc_auc_score', xgb_roc_auc_score)\n",
    "  vizualize_roc_curves(xgb_roc_auc_score, random_roc_auc_score, y_test, xgb_probs, random_probs)\n",
    "  tn, fp, fn, tp = conf_matrix.ravel()\n",
    "  mlflow.log_metric('true_negatives', tn)\n",
    "  mlflow.log_metric('false_positives', fp)\n",
    "  mlflow.log_metric('false_negatives', fn)\n",
    "  mlflow.log_metric('true_positives', tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "DpBgmHX9dXcv",
   "metadata": {
    "id": "DpBgmHX9dXcv"
   },
   "outputs": [],
   "source": [
    "def track_model_version(clf):\n",
    "  \"\"\"Version model on MLFlow UI.\n",
    "\n",
    "  Keyword arguments:\n",
    "  clf -- classifier model\n",
    "  \"\"\"\n",
    "  tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "  if tracking_url_type_store != 'file':\n",
    "      mlflow.sklearn.log_model(clf, 'model', registered_model_name='XGBClassifier')\n",
    "  else:\n",
    "      mlflow.sklearn.log_model(clf, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bNg-yhC7uSeS",
   "metadata": {
    "id": "bNg-yhC7uSeS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_mlflow_run_tags():\n",
    "  \"\"\"Set current MLFlow run tags.\"\"\"\n",
    "  tags = {'model_name': 'XGBClassifier'}\n",
    "  mlflow.set_tags(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e862b8bd",
   "metadata": {
    "id": "e862b8bd"
   },
   "outputs": [],
   "source": [
    "def train_and_track_model_in_mlflow():\n",
    "  \"\"\"Train model and track it with MLFLow\"\"\"\n",
    "  (X_train, X_test, y_train, y_test) = get_split_train_data()\n",
    "  logger = get_configured_logger()\n",
    "  clf = train_xgboost_classifier(X_train, y_train)\n",
    "  with mlflow.start_run():\n",
    "    track_model_params(clf)\n",
    "    track_model_metrics(clf, X_test, y_test)\n",
    "    track_model_version(clf)\n",
    "    set_mlflow_run_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "-e1kKCzVuSeT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-e1kKCzVuSeT",
    "outputId": "66804a05-b021-43c8-db76-a249c2185edd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49341, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\wiemc\\documents\\applications_of_big_data\\home-credit\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "20 fits failed out of a total of 20.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\wiemc\\documents\\applications_of_big_data\\home-credit\\venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\users\\wiemc\\documents\\applications_of_big_data\\home-credit\\venv\\lib\\site-packages\\xgboost\\core.py\", line 506, in inner_f\n",
      "    return f(**kwargs)\n",
      "  File \"c:\\users\\wiemc\\documents\\applications_of_big_data\\home-credit\\venv\\lib\\site-packages\\xgboost\\sklearn.py\", line 1250, in fit\n",
      "    self._Booster = train(\n",
      "  File \"c:\\users\\wiemc\\documents\\applications_of_big_data\\home-credit\\venv\\lib\\site-packages\\xgboost\\training.py\", line 188, in train\n",
      "    bst = _train_internal(params, dtrain,\n",
      "  File \"c:\\users\\wiemc\\documents\\applications_of_big_data\\home-credit\\venv\\lib\\site-packages\\xgboost\\training.py\", line 81, in _train_internal\n",
      "    bst.update(dtrain, i, obj)\n",
      "  File \"c:\\users\\wiemc\\documents\\applications_of_big_data\\home-credit\\venv\\lib\\site-packages\\xgboost\\core.py\", line 1680, in update\n",
      "    _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n",
      "  File \"c:\\users\\wiemc\\documents\\applications_of_big_data\\home-credit\\venv\\lib\\site-packages\\xgboost\\core.py\", line 218, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "xgboost.core.XGBoostError: Some trailing characters could not be parsed: '    1.0\n",
      "12354    1.0\n",
      "24465    1.0\n",
      "17654    1.0\n",
      "23177    1.0\n",
      "        ... \n",
      "65184    1.0\n",
      "23109    1.0\n",
      "58835    1.0\n",
      "54434    1.0\n",
      "55217    1.0\n",
      "Length: 49341, dtype: float64'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\users\\wiemc\\documents\\applications_of_big_data\\home-credit\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan]\n",
      "  warnings.warn(\n",
      "c:\\users\\wiemc\\documents\\applications_of_big_data\\home-credit\\venv\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\users\\wiemc\\documents\\applications_of_big_data\\home-credit\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:98: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\users\\wiemc\\documents\\applications_of_big_data\\home-credit\\venv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "Some trailing characters could not be parsed: '    1.0\n12354    1.0\n24465    1.0\n17654    1.0\n23177    1.0\n        ... \n65184    1.0\n23109    1.0\n58835    1.0\n54434    1.0\n55217    1.0\nLength: 49341, dtype: float64'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mXGBoostError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_16132/2906696064.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtrain_and_track_model_in_mlflow\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_16132/1840818801.py\u001B[0m in \u001B[0;36mtrain_and_track_model_in_mlflow\u001B[1;34m()\u001B[0m\n\u001B[0;32m      3\u001B[0m   \u001B[1;33m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX_test\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_test\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_split_train_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m   \u001B[0mlogger\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_configured_logger\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m   \u001B[0mclf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_xgboost_classifier\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m   \u001B[1;32mwith\u001B[0m \u001B[0mmlflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstart_run\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[0mtrack_model_params\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mclf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_16132/1295983335.py\u001B[0m in \u001B[0;36mtrain_xgboost_classifier\u001B[1;34m(X_train, y_train)\u001B[0m\n\u001B[0;32m     13\u001B[0m                   n_jobs=-1, cv=5, scoring=\"accuracy\")\n\u001B[0;32m     14\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 15\u001B[1;33m   \u001B[0mgs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     16\u001B[0m   \u001B[0mclf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_params\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m**\u001B[0m\u001B[0mgs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbest_params_\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m   \u001B[0mclf\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mclf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\wiemc\\documents\\applications_of_big_data\\home-credit\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, X, y, groups, **fit_params)\u001B[0m\n\u001B[0;32m    924\u001B[0m             \u001B[0mrefit_start_time\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    925\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0my\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 926\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbest_estimator_\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    927\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    928\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbest_estimator_\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\wiemc\\documents\\applications_of_big_data\\home-credit\\venv\\lib\\site-packages\\xgboost\\core.py\u001B[0m in \u001B[0;36minner_f\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    504\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mk\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0marg\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparameters\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    505\u001B[0m             \u001B[0mkwargs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mk\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0marg\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 506\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    507\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    508\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0minner_f\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\wiemc\\documents\\applications_of_big_data\\home-credit\\venv\\lib\\site-packages\\xgboost\\sklearn.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001B[0m\n\u001B[0;32m   1248\u001B[0m         )\n\u001B[0;32m   1249\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1250\u001B[1;33m         self._Booster = train(\n\u001B[0m\u001B[0;32m   1251\u001B[0m             \u001B[0mparams\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1252\u001B[0m             \u001B[0mtrain_dmatrix\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\wiemc\\documents\\applications_of_big_data\\home-credit\\venv\\lib\\site-packages\\xgboost\\training.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001B[0m\n\u001B[0;32m    186\u001B[0m     \u001B[0mBooster\u001B[0m \u001B[1;33m:\u001B[0m \u001B[0ma\u001B[0m \u001B[0mtrained\u001B[0m \u001B[0mbooster\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    187\u001B[0m     \"\"\"\n\u001B[1;32m--> 188\u001B[1;33m     bst = _train_internal(params, dtrain,\n\u001B[0m\u001B[0;32m    189\u001B[0m                           \u001B[0mnum_boost_round\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnum_boost_round\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    190\u001B[0m                           \u001B[0mevals\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mevals\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\wiemc\\documents\\applications_of_big_data\\home-credit\\venv\\lib\\site-packages\\xgboost\\training.py\u001B[0m in \u001B[0;36m_train_internal\u001B[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001B[0m\n\u001B[0;32m     79\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbefore_iteration\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbst\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtrain\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevals\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     80\u001B[0m             \u001B[1;32mbreak\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 81\u001B[1;33m         \u001B[0mbst\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdtrain\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mobj\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     82\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mafter_iteration\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mbst\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtrain\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mevals\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     83\u001B[0m             \u001B[1;32mbreak\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\wiemc\\documents\\applications_of_big_data\\home-credit\\venv\\lib\\site-packages\\xgboost\\core.py\u001B[0m in \u001B[0;36mupdate\u001B[1;34m(self, dtrain, iteration, fobj)\u001B[0m\n\u001B[0;32m   1678\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1679\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mfobj\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1680\u001B[1;33m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001B[0m\u001B[0;32m   1681\u001B[0m                                                     \u001B[0mctypes\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mc_int\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0miteration\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1682\u001B[0m                                                     dtrain.handle))\n",
      "\u001B[1;32mc:\\users\\wiemc\\documents\\applications_of_big_data\\home-credit\\venv\\lib\\site-packages\\xgboost\\core.py\u001B[0m in \u001B[0;36m_check_call\u001B[1;34m(ret)\u001B[0m\n\u001B[0;32m    216\u001B[0m     \"\"\"\n\u001B[0;32m    217\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mret\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 218\u001B[1;33m         \u001B[1;32mraise\u001B[0m \u001B[0mXGBoostError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpy_str\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_LIB\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mXGBGetLastError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    219\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    220\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mXGBoostError\u001B[0m: Some trailing characters could not be parsed: '    1.0\n12354    1.0\n24465    1.0\n17654    1.0\n23177    1.0\n        ... \n65184    1.0\n23109    1.0\n58835    1.0\n54434    1.0\n55217    1.0\nLength: 49341, dtype: float64'"
     ]
    }
   ],
   "source": [
    "train_and_track_model_in_mlflow()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "2.0-cbw-xgboost-train.ipynb",
   "provenance": []
  },
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}