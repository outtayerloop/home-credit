{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import logging\n",
    "import mlflow\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_split_train_data():\n",
    "  \"\"\"Return a tuple containing split train data into X_train X_test, y_train and y_test.\"\"\"\n",
    "  df = pd.read_csv('../../data/processed/processed_application_train.csv')\n",
    "  train, test = train_test_split(df)\n",
    "  X_train = train.drop(['TARGET'], axis=1)\n",
    "  X_test = test.drop(['TARGET'], axis=1)\n",
    "  y_train = train[['TARGET']]\n",
    "  y_test = test[['TARGET']]\n",
    "  return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding MLFLow workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_configured_logger():\n",
    "  \"\"\"Return a logger for console outputs configured to print warnings.\"\"\"\n",
    "  logging.basicConfig(level=logging.WARN)\n",
    "  return logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model on split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_random_forest_classifier(X_train, y_train):\n",
    "  \"\"\"Return RandomForestClassifier fit on input ndarrays X_train and y_train.\n",
    "\n",
    "  Keyword arguments:\n",
    "  X_train -- ndarray containing all train columns except target column\n",
    "  y_train -- ndarray target column values to train the model\n",
    "  \"\"\"\n",
    "  clf = RandomForestClassifier(class_weight='balanced', n_estimators=100)\n",
    "  grid_search = GridSearchCV(clf, {'max_depth': [10, 15], 'min_samples_split': [5, 10]}, n_jobs=-1, cv=5, scoring='accuracy')\n",
    "  grid_search.fit(X_train.values, y_train.values)\n",
    "  clf.set_params(**grid_search.best_params_)\n",
    "  clf = clf.fit(X_train, y_train)\n",
    "  return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting model evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def eval_metrics(actual, pred):\n",
    "  \"\"\"Return a tuple containing model classification accuracy, confusion matrix and f1_score.\n",
    "\n",
    "  Keyword arguments:\n",
    "  actual -- ndarray y_test containing true target values\n",
    "  pred -- ndarray of the predicted target values by the model\n",
    "  \"\"\"\n",
    "  accuracy = accuracy_score(actual, pred)\n",
    "  conf_matrix = confusion_matrix(actual, pred)\n",
    "  f_score = f1_score(actual, pred)\n",
    "  return accuracy, conf_matrix, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_model_evaluation_metrics(clf, X_test, y_test):\n",
    "  \"\"\"Return a tuple containing model classification accuracy, confusion matrix, f1_score and ROC area under the curve score.\n",
    "\n",
    "  Keyword arguments:\n",
    "  clf -- classifier model\n",
    "  X_test -- ndarray containing all test columns except target column\n",
    "  y_test -- ndarray target column values to test the model\n",
    "  \"\"\"\n",
    "  predicted_repayments = clf.predict(X_test)\n",
    "  (accuracy, conf_matrix, f_score) = eval_metrics(y_test, predicted_repayments)\n",
    "  rf_probs = clf.predict_proba(X_test)\n",
    "  rf_probs = rf_probs[:, 0]  # keeping only the first class (repayment OK)\n",
    "  rf_roc_auc_score = roc_auc_score(y_test, rf_probs)\n",
    "  random_probs = [0 for _ in range(len(y_test))]\n",
    "  random_roc_auc_score = roc_auc_score(y_test, random_probs)\n",
    "  return accuracy, conf_matrix, f_score, rf_roc_auc_score, random_roc_auc_score, rf_probs, random_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking model on MLFLow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def track_model_params(clf):\n",
    "  \"\"\"Log model params on MLFlow UI.\n",
    "\n",
    "  Keyword arguments:\n",
    "  clf -- classifier model\n",
    "  \"\"\"\n",
    "  clf_params = clf.get_params()\n",
    "  for param in clf_params:\n",
    "      param_value = clf_params[param]\n",
    "      mlflow.log_param(param, param_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualizing ROC AUC scores summaries for both Random Forest and Random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def vizualize_roc_curves(rf_roc_auc_score, random_roc_auc_score, y_test, rf_probs, random_probs):\n",
    "  \"\"\"Vizualize ROC curves for both fit model and random model.\n",
    "\n",
    "  Keyword arguments:\n",
    "  rf_roc_auc_score -- fit model ROC AUC score\n",
    "  random_roc_auc_score -- random model ROC AUC score\n",
    "  y_test -- ndarray of target values\n",
    "  rf_probs -- fit model predicted probabilities\n",
    "  random_probs -- random model predicted probabilities\n",
    "  \"\"\"\n",
    "  # summarize scores\n",
    "  print('Random model: ROC AUC=%.3f' % random_roc_auc_score)\n",
    "  print('Random Forest: ROC AUC=%.3f' % rf_roc_auc_score)\n",
    "  # calculate roc curves\n",
    "  random_fpr, random_tpr, _ = roc_curve(y_test, random_probs)\n",
    "  rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_probs)\n",
    "  # plot the roc curve for the model\n",
    "  pyplot.plot(random_fpr, random_tpr, linestyle='--', label='Random model')\n",
    "  pyplot.plot(rf_fpr, rf_tpr, marker='.', label='Random Forest')\n",
    "  # axis labels\n",
    "  pyplot.xlabel('False Positive Rate')\n",
    "  pyplot.ylabel('True Positive Rate')\n",
    "  # show the legend\n",
    "  pyplot.legend()\n",
    "  # show the plot\n",
    "  pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def track_model_metrics(clf, X_test, y_test):\n",
    "  \"\"\"Log model metrics on MLFlow UI.\n",
    "\n",
    "  Keyword arguments:\n",
    "  clf -- classifier model\n",
    "  X_test -- ndarray containing all test columns except target column\n",
    "  y_test -- ndarray target column values to test the model\n",
    "  \"\"\"\n",
    "  (accuracy, conf_matrix, f_score, rf_roc_auc_score, random_roc_auc_score, rf_probs, random_probs) = \\\n",
    "    get_model_evaluation_metrics(clf, X_test, y_test)\n",
    "  mlflow.log_metric('accuracy', accuracy)\n",
    "  mlflow.log_metric('f1_score', f_score)\n",
    "  mlflow.log_metric('roc_auc_score', rf_roc_auc_score)\n",
    "  vizualize_roc_curves(rf_roc_auc_score, random_roc_auc_score, y_test, rf_probs, random_probs)\n",
    "  #mlflow.log_metric('conf_matrix', conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def track_model_version(clf):\n",
    "  \"\"\"Version model on MLFlow UI.\n",
    "\n",
    "  Keyword arguments:\n",
    "  clf -- classifier model\n",
    "  \"\"\"\n",
    "  tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "  if tracking_url_type_store != 'file':\n",
    "      mlflow.sklearn.log_model(clf, 'model', registered_model_name='RandomForestClassifier')\n",
    "  else:\n",
    "      mlflow.sklearn.log_model(clf, 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_mlflow_run_tags():\n",
    "  \"\"\"Set current MLFlow run tags.\"\"\"\n",
    "  tags = {'model_name': 'RandomForestClassifier'}\n",
    "  mlflow.set_tags(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_track_model_in_mlflow():\n",
    "  \"\"\"Train model and track it with MLFLow\"\"\"\n",
    "  (X_train, X_test, y_train, y_test) = get_split_train_data()\n",
    "  logger = get_configured_logger()\n",
    "  clf = train_random_forest_classifier(X_train, y_train)\n",
    "  with mlflow.start_run():\n",
    "    track_model_params(clf)\n",
    "    track_model_metrics(clf, X_test, y_test)\n",
    "    track_model_version(clf)\n",
    "    set_mlflow_run_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_and_track_model_in_mlflow()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}